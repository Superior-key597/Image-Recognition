# NumPy Image Recognition (MNIST)

A modular, from-scratch implementation of a neural network using **only NumPy**.  
No PyTorch, no TensorFlow â€” just mathematics, linear algebra, and code.



## ğŸš€ Why This Project?

This project was built to deeply understand how neural networks work internally, without relying on high-level machine learning frameworks.

Key goals:
- **From Scratch** â€“ Implemented entirely using low-level math and NumPy
- **Modular Design** â€“ Clean separation of model, activations, losses, and training logic
- **Manual Backpropagation** â€“ Explicit chain rule and gradient computation
- **Mini-Batch SGD** â€“ Custom batch generator for stable convergence
- **Visualization** â€“ Automated plotting of loss and accuracy curves
- **He Initialization** â€“ Proper weight initialization for deep network stability



## ğŸ¯ When to Use This Project

- You want to understand the mathematics behind deep learning
- You need a lightweight MLP without heavy dependencies
- You want to see how backpropagation and gradient descent work under the hood
- You are building a portfolio to demonstrate core AI engineering skills



## âš¡ Quick Start

### 1. Clone & Install

```bash
git clone https://github.com/YOUR_USERNAME/Image-Recognition.git
cd Image-Recognition
pip install -r requirements.txt

## 2. Setup Dataset

This project uses the MNIST dataset in CSV format.

- Download `train.csv` from Kaggle (Digit Recognizer)
- Place `train.csv` in the project root directory



## 3. Run Experiments


python experiments.py
```
## ğŸ› ï¸ Features & Tech Stack

**Core Engine:** Python 3.x, NumPy  
**Data Handling:** Pandas  
**Visualization:** Matplotlib  

### Architecture

- Model: Multi-Layer Perceptron (MLP)
- Input: 784 neurons (28Ã—28 pixels)
- Hidden layers: Configurable (e.g. `[128, 64]`)
- Output: 10 neurons (Softmax probabilities)

### Techniques

- Activation: ReLU (hidden), Softmax (output)
- Optimization: Stochastic Gradient Descent (SGD)
- Loss: Categorical Cross-Entropy



## ğŸ“‚ Project Structure
----------
- Image-Recognition/
- â”œâ”€â”€ src/
- â”‚   â”œâ”€â”€ model.py          # NeuralNetwork class & backprop engine
- â”‚   â”œâ”€â”€ utils.py          # Batching, one-hot encoding, accuracy
- â”‚   â”œâ”€â”€ initializers.py   # He uniform initialization
- â”‚   â”œâ”€â”€ activations.py    # ReLU, Softmax & derivatives
- â”‚   â””â”€â”€ losses.py         # Cross-entropy loss
- â”œâ”€â”€ results/              # Generated training plots
- â”œâ”€â”€ train.py              # Training loop
- â”œâ”€â”€ experiments.py        # Experiment runner
- â”œâ”€â”€ requirements.txt
- â””â”€â”€ README.md

## ğŸŒ Results
----------

| Metric              | Performance |
|---------------------|-------------|
| Validation Accuracy | ~96%        |
| Training Loss       | < 0.15      |
| Epochs              | 20          |
| Architecture        | 784 â†’ 128 â†’ 64 â†’ 10 |

Results are based on a train/validation split of the MNIST training set.  
Exact performance may vary depending on initialization and hyperparameters.

* * *

## ğŸ§  What I Learned
----------------

* How backpropagation works at a mathematical level
* Why softmax and cross-entropy gradients simplify
* The impact of weight initialization on training stability
* How to structure ML projects cleanly without frameworks

## ğŸ‘¤ Author
--------

High school student learning artificial intelligence and machine learning.